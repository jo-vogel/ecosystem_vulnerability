# Preprocessing of temperature, soil moisture, fAPAR and volumetric soil water data sets

# Contents ####
###############

# Crop to Mediterranean extent
# Rescale to common spatial resolution
# Monthly Aggregation
# Deseasonalisation
# Z-score calculation
# Calculation of percentiles


path_data <- message("insert path here")
path_workspaces <- message("insert path here")
path_data <- "D:/Local/Data/"
path_workspaces <- "C:/Users/vogel/boxup/Promotion/Impact_analysis/Code/Workspaces/"

library(raster)
library(tictoc)
library(pbapply)
library(ncdf4)
library(rasterVis)
library(rworldxtra)
library(rgdal)

# Preprocessing ####
fapar_scene <- raster(paste0(path_data,"fAPAR/fAPAR_extracted/c_gls_FAPAR_199901100000_GLOBE_VGT_V2.0.2/19990110/c_gls_FAPAR-FAPAR_199901100000_CUSTOM_VGT_V2.0.2.tiff"))
sm_scene <- raster(paste0(path_data,"ESA_CCI/2_daily_images/combined/1999/ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-19990101000000-fv04.7.nc"),varname="sm")
sm_crop <- crop(sm_scene,extent(fapar_scene))



# Temperature ####
##################
run_this <- F # takes some time therefore the result is loaded to not run it always again
if (run_this) {
  # Load nc file as raster brick ####
  # ERA5 Land
  temp_brick <- brick(paste0(path_data,"ERA5/Monthly/2m_air_temperature/adaptor.mars.internal-1588056824.0637422-31117-11-36cb0742-c1eb-4054-88d0-668b61bcd246.nc"),varname="t2m",level=1) 
  
  
  # Crop to Mediterranean extent ####
  
  # Problem here: Spain is missing due to pacific-based map cutoff
  # Make two separate crops, shift the western crop once around the globe, then merge them
  temp_crop_east <- crop(temp_brick,extent(sm_crop))
  temp_crop_west <- crop(temp_brick,c(345,359.75 ,26.80804,48.02232))
  temp_crop_west <- shift(temp_crop_west,dx=-359.7) # ERA5 Land
  temp_crop <- merge(temp_crop_west,temp_crop_east)
  # plot(temp_crop[[40]])
  
  
  # Resample to resolution of soil moisture
  
  # temp_brick_res <- resample(temp_crop,sm_crop)
  # message('this also crops automatically')
  # writeRaster(temp_brick_res,filename = paste0(path_workspaces,"temp_brick_rescaled_ERA5Land.tif"), overwrite=T)
}
temp_brick_res <- brick(paste0(path_workspaces,"temp_brick_rescaled_ERA5Land.tif"))

# Pick reference time for deseasonalisation and percentile calculation
span <- length(1998:2019) # 21 Jahre Zeitraum mit Datenverfügbarkeit aller Datensätze + 1 vorangegangenes Jahr für time lag

# Deseasonalise ####

# transform to matrix
temp_matrix <- values(temp_brick_res)
# temp_matrix <- raster::as.data.frame(temp_brick_res) # alternative approach
# Time span reduction
temp_months <- ((39*12)-span*12+1):(39*12) # 39 years * 12 months
temp_matrix <- temp_matrix[,temp_months] # reduce to 1998:2019
interval_month_temp <- rep(c(1,2,3,4,5,6,7,8,9,10,11,12),span) # ERA5 Land


deseason <- function(dat_abs, interval){
  ts_dates <- rbind(dat_abs,interval)
  # Calculation of the mean value of each date for all pixels
  mean_dat <- pbsapply(1:dim(dat_abs)[1], function (x){ tapply(ts_dates[x,],INDEX=ts_dates[dim(dat_abs)[1]+1,], FUN=mean, na.rm=T)})
  # Extract the corresponding mean of the respective date in to steps: 1.get date (month 1-12), 2. Assign the respective mean value
  dat_averages_date <- pbsapply(1:dim(dat_abs)[1], function (y) {sapply(1:dim(ts_dates)[2],function(x){mean_dat[ts_dates[dim(dat_abs)[1]+1,x],y]})}) # last column: column with the dates
  # Subtract temporal means
  return(ts_dates[1:dim(dat_abs)[1],]- t(dat_averages_date))
}

temp_deseason <- deseason(temp_matrix, interval_month_temp)


# Calculate z-scores ####

temp_des_z <- t(apply(temp_deseason,1,scale)) # z-scores on entire deseasonalised time series

# create array
temp_array <- array(temp_matrix,dim=c(dim(temp_matrix)[1],12,span)) # 19975 pixel, 12 months, span years
temp_array <- aperm(temp_array,perm=c(1,3,2)) # entries get filled by rows (so rows need to be months first), then you can rearrange and shift months to columns for scale-command
temp_z_score1 <- pblapply(1:dim(temp_matrix)[1], function(x) scale(temp_array[x,,]) ) # ordered by months
temp_z_score2 <- t(pbsapply(1:dim(temp_matrix)[1], function(x) as.vector(t(temp_z_score1[[x]])) )) # continuous time series


# Calculation of percentiles
percentiles_temp_ds <- apply(temp_deseason,1,function(x) {quantile(x, probs = seq(0.85,0.95,0.05),na.rm=T)}) # for individual pixels
percentiles_temp_des_z <- apply(temp_des_z,1,function(x) {quantile(x, probs = seq(0.85,0.95,0.05),na.rm=T)}) # for individual pixels
percentiles_temp_zsc <- apply(temp_z_score2,1,function(x) {quantile(x, probs = seq(0.85,0.95,0.05),na.rm=T)}) # for individual pixels
percentiles_temp_abs <- apply(temp_matrix,1,function(x) {quantile(x, probs = seq(0.85,0.95,0.05),na.rm=T)}) # for individual pixels

# Extract last 21 years from time span of 30 years from 1990 - 2019 
temp_deseason99 <- temp_deseason[,((span - length(1999:2019))*12+1):dim(temp_matrix)[2]]
temp_des_z_99 <- temp_des_z[,((span - length(1999:2019))*12+1):dim(temp_matrix)[2]]
temp_z_score99 <- temp_z_score2[,((span - length(1999:2019))*12+1):dim(temp_matrix)[2]]
temp_matrix99 <- temp_matrix[,((span - length(1999:2019))*12+1):dim(temp_matrix)[2]]  



# fAPAR ####
############

if (run_this) { # takes some time therefore the result is loaded to not run it always again
  # Load fAPAR data as raster stack
  fapar_all <- list.files(path=paste0(path_data,"fAPAR/fAPAR_extracted/"), pattern="FAPAR(?:-|-RT6-)FAPAR", recursive = T)
  fapar_allnot <- list.files(path=paste0(path_data,"fAPAR/fAPAR_extracted/"), pattern="FAPAR(?:-|-RT6-)FAPAR_2020", recursive = T) # year 2020
  fapar_all <- fapar_all[!(fapar_all %in% fapar_allnot)] # exclude 2020
  fap_files <- paste0(paste0(path_data,"fAPAR/fAPAR_extracted/"),fapar_all)
  fap_stack <- stack(fap_files)
  
    # Resample to resolution of soil moisture ####
  
  # fap_resc <- resample(fap_stack,sm_crop)
  # writeRaster(fap_resc,filename = "./Code/Workspaces/fapar_brick_rescaled.tif")
  fap_resc <- brick(paste0(path_workspaces,"fapar_brick_rescaled.tif"))
  
  # Aggregate dekads to months
  # vec_dekad <- seq(1,754,3)
  # fapar_monthly <- pbsapply(1:252, function(x) mean(fap_resc[[vec_dekad[x]]],fap_resc[[vec_dekad[x]+1]],fap_resc[[vec_dekad[x]+2]]))
  # fapar_monthly_brick <- brick(fapar_monthly)
  # writeRaster(fapar_monthly_brick, filename=paste0(path_workspaces,"fapar_monthly_brick.tif"))
}
fapar_monthly_brick <- brick(paste0(path_workspaces,"fapar_monthly_brick.tif"))


# Deseasonalise ####

# transform to matrix
fapar_matrix <- values(fapar_monthly_brick)
interval_month_fapar <- rep(c(1,2,3,4,5,6,7,8,9,10,11,12),21)
fapar_deseason <- deseason(fapar_matrix, interval_month_fapar)


# Calculate z-scores ####

fapar_des_z <- t(apply(fapar_deseason,1,scale)) # z-scores on entire deseasonalised time series

# create array
fapar_array <- array(fapar_matrix,dim=c(dim(fapar_matrix)[1],12,21)) # 19975 pixel, 12 months, 21 years
fapar_array <- aperm(fapar_array,perm=c(1,3,2)) # entries get filled by rows (so rows need to be months first), then you can rearrange and shift months to columns for scale-command
fapar_z_score1 <- pblapply(1:dim(fapar_matrix)[1], function(x) scale(fapar_array[x,,]) ) # ordered by months
fapar_z_score2 <- t(pbsapply(1:dim(fapar_matrix)[1], function(x) as.vector(t(fapar_z_score1[[x]])) )) # continuous time series


# Calculation of percentiles
percentiles_fapar_ds <- apply(fapar_deseason,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_fapar_des_z <- apply(fapar_des_z,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_fapar_zsc <- apply(fapar_z_score2,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_fapar_abs <- apply(fapar_matrix,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
message('inverse order of percentiles so they are sorted from less to more extreme like temperature')



# Soil moisture ####
####################

message("The R code yields the same results in preprocessing as CDO, but we will continue using CDO for its effectiveness and higher suitability.")

if (run_this) {# takes some time therefore the result is loaded to not run it always again
  # # compile 1 big soil moisture file
  # sm <- list.files(paste0(path_data,"ESA_CCI/2_daily_images/combined/"),pattern=".nc",recursive=T)
  # sm_files <- paste0(path_data,"ESA_CCI/2_daily_images/combined/",sm)
  # sm_stack <- stack(sm_files,varname="sm") # takes 7 minutes
  # 
  # # Crop to Mediterranean extent
  # # sm_stack_crop3000 <- crop(sm_stack[[1:3000]],extent(sm_crop)) # takes 6 minutes
  # # sm_stack_crop6000 <- crop(sm_stack[[3001:6000]],extent(sm_crop)) # takes 6 minutes
  # # sm_stack_crop6000 <- crop(sm_stack[[3001:6000]],extent(sm_crop)) # takes 6 minutes
  # # sm_stack_crop9000 <- crop(sm_stack[[6001:9000]],extent(sm_crop)) # takes 6 minutes
  # # sm_stack_crop12000 <- crop(sm_stack[[9001:12000]],extent(sm_crop)) # takes 6 minutes
  # # sm_stack_crop15036 <- crop(sm_stack[[12001:15036]],extent(sm_crop)) # takes 6 minutes
  # sm_stack_all <- stack(sm_stack_crop3000,sm_stack_crop6000,sm_stack_crop9000,sm_stack_crop12000,sm_stack_crop15036) # takes 43 seconds
  # # writeRaster(sm_stack_all,filename = paste0(path_workspaces,"sm_stack.tif")) # takes 5 min
  # sm_brick <- brick(paste0(path_workspaces,"sm_stack.tif"))
  # 
  # 
  # # Aggregate monthly
  # months <- rep(c(31,28,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31,31,29,31,30,31,30,31,31,30,31,30,31,31,28,31,30,31,30,31,31,30,31,30,31),11)[1:(42*12)] # 1978-2019
  # sum(months)-length(sm) # 1978 has only 61 entries (Nov and Dec), 304 are missing
  # months <- months[13:504] # delete 1978 (no complete year)
  # vec_days <- c(1,cumsum(months)+1)+61 # start at entry 62 (in order to omit the 61 entries from 1978)
  # tic()
  # sm_monthly <- pbsapply(1:492, function(x) mean(sm_brick[[vec_days[x]:(vec_days[x+1]-1)]],na.rm=T)) # takes 31 hours
  # toc()
  # sm_monthly_brick <- brick(sm_monthly)
  # # writeRaster(sm_monthly_brick,filename = paste0(path_workspaces,"sm_monthly.tif"))
  
  # # sm_monthly_brick <- brick(paste0(path_workspaces,"sm79_19_monthly.tif")) # created with R
}
sm_monthly_brick <- brick(paste0(path_workspaces,"sm79_19_monthly.nc"), varname="sm") # created with cdo


# Deseasonalise ####
# transform to matrix
sm_matrix <- values(sm_monthly_brick)
# Time span reduction
sm_months <- ((41*12)-span*12+1):(41*12)
sm_matrix <- sm_matrix[,sm_months] # reduce to 1998:2019
interval_month_sm <- rep(c(1,2,3,4,5,6,7,8,9,10,11,12),span)

sm_deseason <- deseason(sm_matrix, interval_month_sm)


# Calculate z-scores ####

sm_des_z <- t(apply(sm_deseason,1,scale)) # z-scores on entire deseasonalised time series

# create array
sm_array <- array(sm_matrix,dim=c(dim(sm_matrix)[1],12,span)) # 19975 pixel, 12 months, span years
sm_array <- aperm(sm_array,perm=c(1,3,2)) # entries get filled by rows (so rows need to be months first), then you can rearrange and shift months to columns for scale-command
sm_z_score1 <- pblapply(1:dim(sm_matrix)[1], function(x) scale(sm_array[x,,]) ) # ordered by months
sm_z_score2 <- t(pbsapply(1:dim(sm_matrix)[1], function(x) as.vector(t(sm_z_score1[[x]])) )) # continuous time series


# Calculation of percentiles
percentiles_sm_ds <- apply(sm_deseason,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_sm_des_z <- apply(sm_des_z,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_sm_zsc <- apply(sm_z_score2,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_sm_abs <- apply(sm_matrix,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
message('inverse order of percentiles so they are sorted from less to more extreme like temperature')

# Extract last 21 years from time span of 30 years from 1990 - 2019 
sm_deseason99 <- sm_deseason[,((span - length(1999:2019))*12+1):dim(sm_matrix)[2]]
sm_des_z_99 <- sm_des_z[,((span - length(1999:2019))*12+1):dim(sm_matrix)[2]]
sm_z_score99 <- sm_z_score2[,((span - length(1999:2019))*12+1):dim(sm_matrix)[2]]
sm_matrix99 <- sm_matrix[,((span - length(1999:2019))*12+1):dim(sm_matrix)[2]]



# Volumetric soil water ERA5 Land ####
##################
if (run_this) {
  # Load nc file as raster brick ####
  # ERA5 Land
  vsw_brick <- brick(paste0(path_data,"ERA5/Monthly/Volumetric soil water/adaptor.mars.internal-1595857503.435754-10275-29-f27e789b-af7c-454e-ae9b-2bc2d4ba2459.nc"), varname="swvl1") # Volumetric soil water layer 1, 0 - 7cm
  # vsw_brick <- brick(paste0(path_data,"ERA5/Monthly/Volumetric soil water/adaptor.mars.internal-1595863167.782652-32065-13-4ec49959-4faa-4365-8192-f57bd1daaf8f.nc"), varname="swvl2") # Volumetric soil water layer 2, 7 - 28cm
  # vsw_brick <- brick(paste0(path_data,"ERA5/Monthly/Volumetric soil water/adaptor.mars.internal-1595916732.9311278-9757-7-b600cc2f-c80f-486b-85c9-4fb7d8eb692d.nc"), varname="swvl3") # Volumetric soil water layer 3, 28 - 100cm
  # vsw_brick <- brick(paste0(path_data,"ERA5/Monthly/Volumetric soil water/adaptor.mars.internal-1595917667.9937005-27194-25-62cc3239-9e98-4a77-a6a2-ad0e7bbe3e1d.nc"), varname="swvl4") # Volumetric soil water layer 4, 100 - 289cm

  # Crop to Mediterranean extent ####
  
  # problem here: Spain is missing due to pacific-based map cutoff
  # Make two separate crops, shift the western crop once around the globe, then merge them
  vsw_crop_east <- crop(vsw_brick,extent(sm_crop))
  vsw_crop_west <- crop(vsw_brick,c(345,359.75 ,26.80804,48.02232))
  vsw_crop_west <- shift(vsw_crop_west,dx=-359.7) # ERA5 Land
  vsw_crop <- merge(vsw_crop_west,vsw_crop_east)
  # plot(vsw_crop[[40]])
  
  
  # Resample to resolution of soil moisture
  
  # vsw_brick_res <- resample(vsw_crop, sm_crop)
  # message('this also crops automatically')
  # writeRaster(vsw_brick_res, filename = paste0(path_workspaces,"vsw_brick_rescaled_ERA5Land.tif"),overwrite=T)
}
vsw_brick_res <- brick(paste0(path_workspaces,"vsw1_brick_rescaled_ERA5Land.tif"))

# Deseasonalise ####

# transform to matrix
vsw_matrix <- values(vsw_brick_res)
# Time span reduction
vsw_months <- ((39*12)-span*12+1):(39*12) 
vsw_matrix <- vsw_matrix[,vsw_months] # reduce to 1998:2019
interval_month_vsw <- rep(c(1,2,3,4,5,6,7,8,9,10,11,12),span) # ERA5 Land

vsw_deseason <- deseason(vsw_matrix, interval_month_vsw)


# Calculate z-scores ####

vsw_des_z <- t(apply(vsw_deseason,1,scale)) # z-scores on entire deseasonalised time series

# create array
vsw_array <- array(vsw_matrix,dim=c(dim(vsw_matrix)[1],12,span)) # 19975 pixel, 12 months, span years
vsw_array <- aperm(vsw_array,perm=c(1,3,2)) # entries get filled by rows (so rows need to be months first), then you can rearrange and shift months to columns for scale-command
vsw_z_score1 <- pblapply(1:dim(vsw_matrix)[1], function(x) scale(vsw_array[x,,]) ) # ordered by months
vsw_z_score2 <- t(pbsapply(1:dim(vsw_matrix)[1], function(x) as.vector(t(vsw_z_score1[[x]])) )) # continuous time series


# Calculation of percentiles
percentiles_vsw_ds <- apply(vsw_deseason,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_vsw_des_z <- apply(vsw_des_z,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_vsw_zsc <- apply(vsw_z_score2,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_vsw_abs <- apply(vsw_matrix,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels

# Extract last 21 years from time span of 30 years from 1990 - 2019 
vsw_deseason99 <- vsw_deseason[,((span - length(1999:2019))*12+1):dim(vsw_matrix)[2]]
vsw_des_z_99 <- vsw_des_z[,((span - length(1999:2019))*12+1):dim(vsw_matrix)[2]]
vsw_z_score99 <- vsw_z_score2[,((span - length(1999:2019))*12+1):dim(vsw_matrix)[2]]
vsw_matrix99 <- vsw_matrix[,((span - length(1999:2019))*12+1):dim(vsw_matrix)[2]]  



# Calculate time lags and corresponding percentiles ####
########################################################

# Calculating 2 and 12 months rolling means
source(paste0(path_workspaces,"Rolling_mean_example.R")) # function for calculating moving average from http://www.cookbook-r.com/Manipulating_data/Calculating_a_moving_average/
temp_mv3 <- t(sapply(1:dim(temp_des_z)[1], function(x) movingAverage(x = temp_des_z[x,], n = 3, centered = F) )) # consider 2 previous and current entry
temp_mv12 <- t(sapply(1:dim(temp_des_z)[1], function(x) movingAverage(x = temp_des_z[x,], n = 12, centered = F) )) # consider 11 previous and current entry
sm_mv3 <- t(sapply(1:dim(sm_des_z)[1], function(x) movingAverage(x = sm_des_z[x,], n = 3, centered = F) )) # consider 2 previous and current entry
sm_mv12 <- t(sapply(1:dim(sm_des_z)[1], function(x) movingAverage(x = sm_des_z[x,], n = 12, centered = F) )) # consider 11 previous and current entry
vsw_mv3 <- t(sapply(1:dim(vsw_des_z)[1], function(x) movingAverage(x = vsw_des_z[x,], n = 3, centered = F) )) # consider 2 previous and current entry
vsw_mv12 <- t(sapply(1:dim(vsw_des_z)[1], function(x) movingAverage(x = vsw_des_z[x,], n = 12, centered = F) )) # consider 11 previous and current entry
fapar_mv3 <- t(sapply(1:dim(fapar_des_z)[1], function(x) movingAverage(x = fapar_des_z[x,], n = 3, centered = F) )) # consider 2 previous and current entry
fapar_mv12 <- t(sapply(1:dim(fapar_des_z)[1], function(x) movingAverage(x = fapar_des_z[x,], n = 12, centered = F) )) # consider 11 previous and current entry
# refill NA values, to avoid creating artifical data
sm_mv3[is.na(sm_des_z)] <- NA
sm_mv12[is.na(sm_des_z)] <- NA

temp_mv3_99 <- temp_mv3[,(dim(temp_des_z)[2]-dim(temp_des_z_99)[2]+1):dim(temp_des_z)[2]]
temp_mv12_99 <- temp_mv12[,(dim(temp_des_z)[2]-dim(temp_des_z_99)[2]+1):dim(temp_des_z)[2]]
sm_mv3_99 <- sm_mv3[,(dim(sm_des_z)[2]-dim(sm_des_z_99)[2]+1):dim(sm_des_z)[2]]
sm_mv12_99 <- sm_mv12[,(dim(sm_des_z)[2]-dim(sm_des_z_99)[2]+1):dim(sm_des_z)[2]]
vsw_mv3_99 <- vsw_mv3[,(dim(vsw_des_z)[2]-dim(vsw_des_z_99)[2]+1):dim(vsw_des_z)[2]]
vsw_mv12_99 <- vsw_mv12[,(dim(vsw_des_z)[2]-dim(vsw_des_z_99)[2]+1):dim(vsw_des_z)[2]]
percentiles_temp_des_z_mv3 <- apply(temp_mv3,1,function(x) {quantile(x, probs = seq(0.85,0.95,0.05),na.rm=T)}) # for individual pixels
percentiles_temp_des_z_mv12 <- apply(temp_mv12,1,function(x) {quantile(x, probs = seq(0.85,0.95,0.05),na.rm=T)}) # for individual pixels
percentiles_sm_des_z_mv3 <- apply(sm_mv3,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_sm_des_z_mv12 <- apply(sm_mv12,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_vsw_des_z_mv3 <- apply(vsw_mv3,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_vsw_des_z_mv12 <- apply(vsw_mv12,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_fapar_des_z_mv3 <- apply(fapar_mv3,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels
percentiles_fapar_des_z_mv12 <- apply(fapar_mv12,1,function(x) {quantile(x, probs = seq(0.15,0.05,-0.05),na.rm=T)}) # for individual pixels

percentiles <- list(percentiles_temp_ds, percentiles_temp_des_z, percentiles_temp_abs,percentiles_temp_des_z_mv3, percentiles_temp_des_z_mv12,
                    percentiles_sm_ds, percentiles_sm_des_z, percentiles_sm_abs, percentiles_sm_des_z_mv3, percentiles_sm_des_z_mv12, 
                    percentiles_vsw_ds, percentiles_vsw_des_z, percentiles_vsw_abs, percentiles_vsw_des_z_mv3, percentiles_vsw_des_z_mv12, 
                    percentiles_fapar_ds, percentiles_fapar_des_z,percentiles_fapar_abs, percentiles_fapar_des_z_mv3, percentiles_fapar_des_z_mv12)
names(percentiles) <- c("percentiles_temp_ds", "percentiles_temp_des_z", "percentiles_temp_abs", "percentiles_temp_des_z_mv3", "percentiles_temp_des_z_mv12",
                        "percentiles_sm_ds", "percentiles_sm_des_z", "percentiles_sm_abs", "percentiles_sm_des_z_mv3", "percentiles_sm_des_z_mv12", 
                        "percentiles_vsw_ds", "percentiles_vsw_des_z", "percentiles_vsw_abs", "percentiles_vsw_des_z_mv3", "percentiles_vsw_des_z_mv12",
                        "percentiles_fapar_ds", "percentiles_fapar_des_z", "percentiles_fapar_abs", "percentiles_fapar_des_z_mv3", "percentiles_fapar_des_z_mv12")
temp_list <- list(temp_deseason99, temp_des_z_99, temp_matrix99, temp_mv3_99, temp_mv12_99, temp_deseason, temp_des_z, temp_matrix, temp_mv3, temp_mv12)
names(temp_list) <- c("temp_deseason99", "temp_des_z_99", "temp_matrix99", "temp_mv3_99", "temp_mv12_99","temp_deseason", "temp_des_z", "temp_matrix"," temp_mv3", "temp_mv12")
sm_list <- list(sm_deseason99, sm_des_z_99, sm_matrix99, sm_mv3_99, sm_mv12_99, sm_deseason, sm_des_z, sm_matrix, sm_mv3, sm_mv12)
names(sm_list) <- c("sm_deseason99", "sm_des_z_99", "sm_matrix99", "sm_mv3_99", "sm_mv12_99", "sm_deseason", "sm_des_z", "sm_matrix", "sm_mv3", "sm_mv12")
vsw_list <- list(vsw_deseason99, vsw_des_z_99, vsw_matrix99, vsw_mv3_99, vsw_mv12_99, vsw_deseason, vsw_des_z, vsw_matrix, vsw_mv3, vsw_mv12)
names(vsw_list) <- c("vsw_deseason99", "vsw_des_z_99", "vsw_matrix99", "vsw_mv3_99", "vsw_mv12_99", "vsw_deseason", "vsw_des_z", "vsw_matrix", "vsw_mv3", "vsw_mv12")
fapar_list <- list(fapar_deseason, fapar_des_z, fapar_matrix, fapar_mv3, fapar_mv12)
names(fapar_list) <- c("fapar_deseason", "fapar_des_z", "fapar_matrix", "fapar_mv3", "fapar_mv12")



# Refine area ####
##################

# create Köppen-Geiger map 
source('./Code/Koeppen_Geiger.R') # This file is obtained from http://koeppen-geiger.vu-wien.ac.at/present.htm
# Crop Köppen-Geiger map to Mediterranean area
Koeppen <- crop(Koeppen,sm_crop)
# writeRaster(Koeppen,"./Code/Workspaces/Koeppen_mediterranean.tif")
# Resample Köppen-Geiger map to resolution of ERA 5 data
set.seed(40) # resample gives different result every time
Koeppen <- resample(Koeppen,sm_crop,method="ngb") # nearest neighbor is appropriate for categorical variables
# Apparently, it's not possible to give multiple values to the mask command.
# Workaround: assign both values 12 and 13 to a non-existing value (e.g. 1)
Koeppen[Koeppen@data@values==12] <- 1
Koeppen[Koeppen@data@values==13] <- 1
locat <- which(Koeppen@data@values==1) # all Csa and Csb pixels
locat_land <- which(Koeppen@data@values!=32) # all land pixels (ocean included)


# Save preprocessed data as R-workspace ####
save(locat,locat_land, temp_list, sm_list, vsw_list, fapar_list, percentiles, file=paste0(path_workspaces, "Preprocessed_data.RData"))